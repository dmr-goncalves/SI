<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>

<head>
    <title>Simbrain Documentation</title>
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
    <link href="../../../Styles.css" rel="stylesheet" type="text/css">
</head>

<body>
    <a href="../../../SimbrainDocs.html">
        <div class="logo">
            <p><span></span>
            </p>
        </div>
    </a>
    <div id="main_docs">
        <div class="navi">
            <p><a href="../../../SimbrainDocs.html">Simbrain</a> &gt; <a href="../../Network.html">Network</a> &gt; <a href="../subnetwork.html">Subnetwork</a> &gt; Self-Organizing</p>
        </div>
        <h1>Self-Organizing Map</h1>
        <p>A self-organizing map or SOM is a kind of <a href="competitivenetwork.html">competitive network</a>, which over time is tuned to represent the structure of a set of inputs.&nbsp; For example, an SOM exposed to a dataset consisting of different smells will learn to distinguish those smells over time.&nbsp; Moreover, the positions of the nodes in the SOM are significant: nearby nodes come to represent similar inputs.&nbsp; In a smell network, one group of neighboring nodes might come to represent different cheese smells, while another group might come to represent different flower smells. To get a feel for how SOM's work try the workspaces called "somLetters.zip" and "somSmells.zip".</p>
        <div><img style="float:right; padding-left: 20px; padding-right: 20px;" width="400" src="../../../Images/som.png">
        </div>
        <p>An som may either be created as a group or a network. As a network, it has a self-contained layer of input nodes and it can be trained using a table of inputs. As a group, it is up to the user to connect it to other neurons, and the inputs these produce will determine the way it comes to represent those inputs over time.</p>
        <p class="heading">Algorithm</p>
        <p>Intuitively, the algorithm works by taking an input, finding the output node whose weights most closely match the input (the "winner) and then updating the winning neuron's weights so that they match the inputs more closely.&nbsp; The weights are not only updated on the winning node, but also on other nodes in a neighborhood around the winning node. Over time the learning rate and neighborhood size decrease to 0.&nbsp; Thus the bank of nodes in an SOM network correspond to a kind of "map" of the input space, whereby nearby nodes correspond to similar objects in the network's environment.
        </p>
        <p>The following algorithm is run on each iteration of a SOM network.</p>
        <blockquote>
            <p><span class="heading2">1.</span> Determine the SOM neuron which is closest to the input vector by computing the following for each SOM neuron:
                <br>
            </p>
            <div align="center"><img src="../equations/SOMalgorithm1.png">
                <div>Where i and j are the dimensions of the weight matrix w, and x is the input vector.</div>
            </div>
            <p><span class="heading2">2.</span> Update the winning neuron and the neurons in it's update neighborhood:
                <br>
            </p>
            <div align="center"><img src="../equations/SOMalgorithm2.png">
                <div>Where &#945; is the current learning rate.</div>
            </div>
            <p><span class="heading2">3.</span> Diminish learning rate and neighborhood size.
                <br>
            </p>
        </blockquote>
        <p>The effect of the algorithm is such that the SOM neurons that remain are characteristic of the trends of input patterns.</p>
        <p class="heading">Training</p>
        <blockquote>
            <p>Training a network involves specifying a set of input data and then running the algorithm. This will repeatedly apply the SOM algorithm. The general process process is covered <a href="../training.html">here</a>. The decreasing learning rate and neighborhood size are shown in the interaction box. Occasionally it may be useful to reset the network to try training it again, possible with modified parameters.
            </p>
        </blockquote>

        <p class="heading">Creation</p>
        <blockquote>
            <p>SOMs are initialized by specifying a number of neurons and a layout for those neurons. The layout is important, because the SOM works by updating a winning node and neighboring nodes.&nbsp;
            </p>
            <p>Input vectors are activations of neurons connecting to the SOM network,&nbsp; and should be fully connected to the SOM network.</p>
            <p>The synapses should be either small or sampled evenly from the subspace spanned by the two largest <a href="http://en.wikipedia.org/wiki/Principal_component">principal component</a> eigenvectors.
            </p>
            <div><img style="float:left;" width="400" src="../../../Images/som_dialog.png">
            </div>
        </blockquote>


        <p class="heading">Training the SOM</p>
        <blockquote>
            <p>When used as a group only, or when linked to external networks or sources of data, then those inputs train the network, and it is only up to the user to occasionally reset the parameters and the network. </p>

            <p>When the <a href="../training/trainingDialog.html">training dialog</a>   is used inputs can be specified in the input tab, and then the play button pressed until the learning rate and neighborhood size go to 0.  Parameters controlling the rate at which these decay can be adjusted and learning re-run to try to achieved a desired result.</p> 

        </blockquote>

        <p class="heading">Parameters</p>
        <blockquote>
            <p><span class="heading2">Number of SOM Neurons: </span>Sets number of neurons for the network. </p>
            <p><span class="heading2">Number of Input Neurons: </span>Sets the number of onput neurons. </p>
            <p><span class="heading2">Initial Learning Rate: </span>The base learning rate from which all future learning rates are derived. Usually not equal to zero.</p>
            <p><span class="heading2">Initial Neighborhood Size: </span>The base Neighborhood Size from which all future neighborhood sizes are derived.</p>
            <p><span class="heading2">Learning Decay Rate:</span> The proportion by which the learning rate is decreased after each iteration. </p>
            <p><span class="heading2">Neighborhood Decay Amount:</span> The amount that Neighborhood Size decreases after each iteration. </p>
            <p><span class="heading2">Update Interval:</span> This is the interval that the Learning Decay Rate and Neighborhood Decay Amount is updated. In most cases, this is equal to the total amount of input vectors.</p>
        </blockquote>

        <p class="heading">Recall</p>
        <blockquote>
            <p>Displays the weights attaching to the most active SOM neuron in the pool of input neurons.&nbsp; This gives a sense of what pattern the currently active SOM node represents in terms of inputs.
                <br>
            </p>
        </blockquote>        

        <p class="heading">Right Click Menu
        </p>
        <blockquote>
            <p>Generic right-click items are described on the <a href="../groups/NeuronGroup.html#rightclick">neuron group</a> page.</p>
            <p><span class="heading2">Edit/Train SOM:</span> Opens edit dialog to train SOM network.
            </p>
            <p> <span class="heading2">Add Current Pattern To Input Data:</span> Add the current pattern in the network to the training set. Useful for creating training data directly in the GUI.</p>
            <p><span class="heading2">Train On Current Pattern: </span>Iterate the training algorithm once using the current inputs.</p>
            <p><span class="heading2">Randomize Synapses: </span> Randomize synapses connected the competitive group, which are the ones trained using the algorithm.
            </p>
        </blockquote>

        <p class="heading">References</p>
        <blockquote>
            <p>Kohonen, Teuvo (1990), The Self Organizing Map,<span style="font-style: italic;"> Proceedings of the IEEE,</span> 78:9.
                <br>
            </p>
        </blockquote>

    </div>
</body>

</html>